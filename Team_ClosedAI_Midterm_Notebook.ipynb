{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "Y7SoC81_XZKv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Suppresses output of this cell.\n",
        "\n",
        "# Install the Unsloth library.\n",
        "!pip install unsloth\n",
        "\n",
        "# Uninstall any existing version and install the latest version of Unsloth from GitHub.\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdtL52OCXZKx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel       # For loading and using language models.\n",
        "import torch                                # PyTorch library for ML tasks.\n",
        "from sklearn.metrics import accuracy_score  # To calculate accuracy.\n",
        "\n",
        "max_seq_length = 2048                       # Max sequence length for the model.\n",
        "dtype = None                                # Auto-detect data type; adjust for specific GPUs if needed.\n",
        "load_in_4bit = True                         # Enable 4-bit quantization to save memory."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the model"
      ],
      "metadata": {
        "id": "kCIeZ6E8U1Bi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Se2tU-IsXZKx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained Llama3-8B model and tokenizer with specified configurations.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B\",   # Specifies the model name.\n",
        "    max_seq_length=max_seq_length,            # Sets the maximum sequence length.\n",
        "    dtype=dtype,                              # Determines data type for computations.\n",
        "    load_in_4bit=load_in_4bit,                # Enables 4-bit quantization if True.\n",
        ")\n",
        "\n",
        "# Set the tokenizer to add padding on the left side of sequences.\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Move the model to the GPU for faster computation.\n",
        "model = model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAxnlOo6XZKx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Apply Parameter-Efficient Fine-Tuning (PEFT) to the model.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=128,      # Rank of LoRA updates; higher values increase expressiveness but use more memory.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # Modules for applying LoRA.\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=128,                         # Scaling factor for LoRA updates.\n",
        "    lora_dropout=0,                         # Dropout rate for LoRA layers; 0 is optimized for performance.\n",
        "    bias=\"none\",                            # Bias handling; \"none\" minimizes memory usage and is optimized.\n",
        "    use_gradient_checkpointing=\"unsloth\",   # Saves memory for long contexts; \"unsloth\" is memory-efficient.\n",
        "    random_state=3407,                      # Sets a random seed for reproducibility.\n",
        "    use_rslora=False,                       # Disables rank-stabilized LoRA; can be enabled if needed.\n",
        "    loftq_config=None,                      # Disables LoftQ quantization; set if quantization is required.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dateset"
      ],
      "metadata": {
        "id": "NRyKt-wiU9ZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T02:19:31.568939Z",
          "iopub.status.busy": "2024-11-13T02:19:31.568549Z",
          "iopub.status.idle": "2024-11-13T02:19:33.171299Z",
          "shell.execute_reply": "2024-11-13T02:19:33.170402Z",
          "shell.execute_reply.started": "2024-11-13T02:19:31.568906Z"
        },
        "id": "Xv7fyGF3XZKx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Download and load the competition dataset from the Hugging Face datasets library.\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")  # Load the specified dataset.\n",
        "\n",
        "# Display the dataset details, such as its structure and content.\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T02:19:57.819762Z",
          "iopub.status.busy": "2024-11-13T02:19:57.819323Z",
          "iopub.status.idle": "2024-11-13T02:19:57.826858Z",
          "shell.execute_reply": "2024-11-13T02:19:57.825860Z",
          "shell.execute_reply.started": "2024-11-13T02:19:57.819718Z"
        },
        "id": "y-3obQADXZKy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define a template for formatting the prompts to include problems, solutions, answers, and outputs.\n",
        "prompt = \"\"\"### Example:\n",
        "\n",
        "Problem:\n",
        "Find the value of y if 4y - 7 = 9.\n",
        "\n",
        "Solution:\n",
        "Adding 7 to both sides, we get 4y = 16. Then, dividing by 4, we find y = 4.\n",
        "\n",
        "Answer:\n",
        "4\n",
        "\n",
        "Output:\n",
        "True\n",
        "\n",
        "Now, evaluate the following problem:\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Solution:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"\n",
        "\n",
        "# Define the End of Sequence (EOS) token to signal the end of each prompt.\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Function to format prompts using the provided template.\n",
        "def formatting_prompts_func(examples):\n",
        "    question = examples[\"question\"]       # The math problem to evaluate.\n",
        "    ans = examples[\"answer\"]              # The given answer to the problem.\n",
        "    output = examples[\"is_correct\"]       # Whether the given answer is correct or not.\n",
        "    explaination = examples[\"solution\"]   # Step-by-step solution to the problem.\n",
        "\n",
        "    texts = []  # List to store formatted texts.\n",
        "    for instruction, input_explaination, input, output in zip(question, explaination, ans, output):\n",
        "        # Format the text using the template and add the EOS token to prevent infinite generation.\n",
        "        text = prompt.format(instruction, input_explaination, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    # Return the formatted texts as a dictionary.\n",
        "    return {\"text\": texts}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train:Valid Split"
      ],
      "metadata": {
        "id": "-xgvs4tGVJKy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T02:20:03.765432Z",
          "iopub.status.busy": "2024-11-13T02:20:03.764757Z",
          "iopub.status.idle": "2024-11-13T02:20:03.814272Z",
          "shell.execute_reply": "2024-11-13T02:20:03.813356Z",
          "shell.execute_reply.started": "2024-11-13T02:20:03.765371Z"
        },
        "id": "xHb9bNJ5XZKy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score  # To evaluate model performance.\n",
        "\n",
        "# Randomly shuffle and sample 110,000 rows from the training dataset.\n",
        "sampled_data = dataset['train'].shuffle(seed=3407).select(range(110000))\n",
        "\n",
        "# Split the sampled data into 100,000 rows for training and 10,000 rows for testing.\n",
        "split_data = sampled_data.train_test_split(test_size=10000, seed=3407)\n",
        "\n",
        "# Assign the split data to training and validation datasets.\n",
        "train_data = split_data['train']            # Training dataset with 100,000 rows.\n",
        "val_dataset = split_data['test']            # Validation dataset with 10,000 rows.\n",
        "\n",
        "# Apply the formatting function to prepare the training dataset for prompt-based learning.\n",
        "train_dataset = train_data.map(formatting_prompts_func, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDoT2XL7XZKy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#print a smaple training example\n",
        "train_dataset['text'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gp6EWbiXZKz"
      },
      "source": [
        "Supervised Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeruJei_XZKz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer                  # Importing the trainer class for Supervised Fine-Tuning.\n",
        "from transformers import TrainingArguments  # Importing the class to define training configurations.\n",
        "from unsloth import is_bfloat16_supported   # Check if bfloat16 is supported for training.\n",
        "\n",
        "# Define the training arguments specifying the configurations for model training.\n",
        "training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=2,        # Batch size for training per device (GPU).\n",
        "        gradient_accumulation_steps=4,        # Accumulate gradients over 4 steps to effectively increase batch size.\n",
        "        warmup_steps=5,                       # Number of steps for learning rate warmup.\n",
        "        max_steps=5000,                       # Maximum number of training steps.\n",
        "        learning_rate=2e-5,                   # Learning rate for model optimization.\n",
        "        fp16=not is_bfloat16_supported(),     # Use fp16 if bfloat16 is not supported.\n",
        "        bf16=is_bfloat16_supported(),         # Use bf16 if supported.\n",
        "        logging_steps=1,                      # Log training metrics every 1 step.\n",
        "        optim=\"adamw_8bit\",                   # Use 8-bit AdamW optimizer for reduced memory usage.\n",
        "        weight_decay=0.001,                   # Weight decay to prevent overfitting.\n",
        "        lr_scheduler_type=\"linear\",           # Linear scheduler for learning rate decay.\n",
        "        seed=3407,                            # Set random seed for reproducibility.\n",
        "        output_dir=\"outputs\",                 # Directory to store model outputs.\n",
        "        report_to=\"none\",                     # Disable logging to WandB or other platforms.\n",
        ")\n",
        "\n",
        "# Initialize the trainer with the provided model, tokenizer, dataset, and training configurations.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,                              # Model to be trained.\n",
        "    tokenizer=tokenizer,                      # Tokenizer for encoding the inputs.\n",
        "    train_dataset=train_dataset,              # Training dataset.\n",
        "    dataset_text_field=\"text\",                # Field name for the text data in the dataset.\n",
        "    max_seq_length=max_seq_length,            # Maximum sequence length for the inputs.\n",
        "    dataset_num_proc=4,                       # Number of processors to use for dataset processing.\n",
        "    packing=False,                            # Disable packing for better efficiency on longer sequences.\n",
        "    args=training_args,                       # Pass the training configurations.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seEFyowgXZKz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()               # Start the training process and store the training statistics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model"
      ],
      "metadata": {
        "id": "uAjHXAj3VZQU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLbhKbKOXZKz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")           # Save the trained model to the local directory \"lora_model\".\n",
        "tokenizer.save_pretrained(\"lora_model\")       # Save the tokenizer to the same local directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knlsRNlKYUIb"
      },
      "outputs": [],
      "source": [
        "!zip -r lora_model.zip lora_model             # Compress the \"lora_model\" directory into a ZIP file named \"lora_model.zip\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fah12Hx-YULF"
      },
      "outputs": [],
      "source": [
        "from google.colab import files                # Import the necessary module for file downloads.\n",
        "files.download(\"lora_model.zip\")              # Trigger the download of the \"lora_model.zip\" file to your local machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaDlfVCDXZK0"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T02:20:17.217458Z",
          "iopub.status.busy": "2024-11-13T02:20:17.216382Z",
          "iopub.status.idle": "2024-11-13T02:20:17.222583Z",
          "shell.execute_reply": "2024-11-13T02:20:17.221587Z",
          "shell.execute_reply.started": "2024-11-13T02:20:17.217387Z"
        },
        "id": "Yv2ghzkaXZK0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel   # Import the FastLanguageModel class from the unsloth library.\n",
        "import torch                            # Import PyTorch for tensor operations and model management.\n",
        "\n",
        "max_seq_length = 2048                   # Set the maximum sequence length for tokenization (can be adjusted based on available resources).\n",
        "dtype = None                            # Set the data type for model weights; None for automatic detection. (e.g., use Float16 for Tesla T4).\n",
        "load_in_4bit = True                     # Enable 4-bit quantization to reduce memory usage (can be set to False if not required)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfjaPdwdzkWr"
      },
      "outputs": [],
      "source": [
        "import zipfile  # Import the zipfile module to handle ZIP file extraction.\n",
        "\n",
        "# Path to the .zip file\n",
        "zip_file_path = \"/content/lora_model.zip\"             # Path to the ZIP file containing the saved model.\n",
        "\n",
        "# Extract the .zip file\n",
        "extracted_folder = \"/content\"                         # Specify the folder to extract the contents (default is \"/content\").\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:  # Open the ZIP file in read mode.\n",
        "    zip_ref.extractall(extracted_folder)              # Extract all files to the specified folder.\n",
        "\n",
        "print(f\"Model extracted to: {extracted_folder}\")      # Print a confirmation message with the folder location.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T02:18:59.524276Z",
          "iopub.status.busy": "2024-11-13T02:18:59.523882Z",
          "iopub.status.idle": "2024-11-13T02:19:12.029175Z",
          "shell.execute_reply": "2024-11-13T02:19:12.028223Z",
          "shell.execute_reply.started": "2024-11-13T02:18:59.524237Z"
        },
        "id": "khcbtzuuXZK1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if True:  # This condition always evaluates to True, so the block will execute.\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\",              # Load the pre-trained model from the \"lora_model\" directory.\n",
        "        max_seq_length = max_seq_length,        # Set the maximum sequence length for the model's inputs.\n",
        "        dtype = dtype,                          # Use the previously defined data type for the model.\n",
        "        load_in_4bit = load_in_4bit,            # Load the model with 4-bit quantization to save memory.\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)      # Optimize the model for inference, making it 2x faster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3nsDCJtaTw6"
      },
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "batch_size = 32                               # Adjust this value based on available GPU memory\n",
        "test_dataset = dataset['test']                # Use the 'test' split of the dataset\n",
        "num_samples = len(test_dataset['question'])   # Set dynamically based on dataset size\n",
        "\n",
        "# Pre-generate all input prompts for the test samples\n",
        "input_prompts = [\n",
        "    prompt.format(ques, ans, sol, \"\")         # \"\" for the output, allowing the model to generate True/False\n",
        "    for ques, ans, sol in zip(test_dataset['question'], test_dataset['answer'], test_dataset['solution'])\n",
        "]\n",
        "\n",
        "# Prepare storage for responses and true labels\n",
        "responses = []                                        # List to store the generated responses\n",
        "true_labels = test_dataset['is_correct']              # Assumes 'is_correct' column contains boolean values indicating correct/incorrect answers\n",
        "\n",
        "# Process in batches\n",
        "for i in range(0, num_samples, batch_size):           # Loop through the dataset in batches\n",
        "    # Select batch of prompts\n",
        "    batch_prompts = input_prompts[i:i + batch_size]   # Get the current batch of prompts\n",
        "\n",
        "    # Tokenize and prepare inputs for the model, move to GPU\n",
        "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")   # Tokenize and send to GPU\n",
        "    input_shape = inputs['input_ids'].shape[1]                                        # The input token length for slicing output\n",
        "\n",
        "    # Run inference\n",
        "    outputs = model.generate(**inputs, max_new_tokens=5, use_cache=True)              # Generate predictions with the model\n",
        "\n",
        "    # Decode each output starting from input length to capture only the generated part\n",
        "    batch_responses = tokenizer.batch_decode(\n",
        "        [output[input_shape:] for output in outputs], skip_special_tokens=True\n",
        "    )  # Decode only the generated tokens, skipping special tokens\n",
        "\n",
        "    # Store the batch responses\n",
        "    responses.extend(batch_responses)               # Append the generated responses to the list\n",
        "\n",
        "    print(f\"Processed batch {i // batch_size + 1}/{(num_samples + batch_size - 1) // batch_size}\")  # Print progress\n",
        "\n",
        "print(\"Inference completed for all samples.\")       # Indicate that all inference is done\n",
        "\n",
        "# At this point, 'responses' contains all generated responses, and 'true_labels' has the true values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R92VKB8LaUAd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd  # Import pandas for DataFrame manipulation\n",
        "\n",
        "# Convert generated responses to boolean values\n",
        "# Assumes `responses` is a list of lists with each inner list containing a single response string\n",
        "# Convert to boolean by checking if it's \"True\"\n",
        "predictions = [resp.strip() == 'True' for resp in responses]  # Strip spaces and check if the response is 'True'\n",
        "\n",
        "# 1) Calculate accuracy\n",
        "accuracy = sum(pred == label for pred, label in zip(predictions, true_labels)) / len(predictions)\n",
        "# Calculate the accuracy by comparing predictions with the true labels and dividing by the total number of samples\n",
        "print(f\"Accuracy: {accuracy:.2f}\")      # Print the accuracy to 2 decimal places\n",
        "\n",
        "# 2) Create a DataFrame and save to CSV\n",
        "df_output = pd.DataFrame({\n",
        "    'ID': range(len(predictions)),  # Create an ID column for each sample\n",
        "    'is_correct': predictions       # Store the predictions in the 'is_correct' column\n",
        "})\n",
        "\n",
        "# Save to CSV in Colab or Kaggle\n",
        "csv_path = 'predictions2.csv'                                       # Define the path where the CSV will be saved\n",
        "df_output.to_csv(csv_path, index=False)                             # Save the DataFrame to a CSV file without including the index\n",
        "\n",
        "print(f\"CSV file saved as '{csv_path}' in the current directory.\")  # Confirm the file is saved successfully\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}